{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a7da52e8",
   "metadata": {},
   "source": [
    "### Name : S.M.Hedayatullah\n",
    "### Enroll No. 0010"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3035915",
   "metadata": {},
   "source": [
    "# Data Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e898da7c",
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (1275136958.py, line 1)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  File \u001b[1;32m\"C:\\Users\\User\\AppData\\Local\\Temp\\ipykernel_19820\\1275136958.py\"\u001b[1;36m, line \u001b[1;32m1\u001b[0m\n\u001b[1;33m    -- Data Cleaning\u001b[0m\n\u001b[1;37m            ^\u001b[0m\n\u001b[1;31mSyntaxError\u001b[0m\u001b[1;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "-- Data Cleaning \n",
    " - Text Cleaning : Removing irrelevant Info, punctuation removal,stop words or special characters.\n",
    " - Changing al text in lower case\n",
    " - stemming \n",
    " - lemmatization\n",
    " - These dont change data into numerical or structural form"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64c8264c",
   "metadata": {},
   "source": [
    "# Other methods of Data Cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fff7380b",
   "metadata": {},
   "outputs": [],
   "source": [
    "1. Bag of Words(BoW) : Represents a Histogram of words , word frwuencies. Histogram is a graph of the frequency of words.\n",
    "- BoW ignores word order and semantics but captures the overall word importance in text\n",
    "\n",
    "2. TF=IDF : Term Frequency- Inverse Document Frequency  \n",
    "    -This builds upon BoW by assigning weights to each word based on its frequency in a document collection\n",
    "    -Words that appear frequently within a single document but rarely across documents get higher weights, \n",
    "    emphasizing their importance for that specific document\n",
    "    \n",
    "3. Word Embeddings: This technique represents words as vectors in higher dimensional space.\n",
    "    - Words in similar space are positioned closer in Vector Space \n",
    "    - Word embeddings capture semantic relationships between words and are useful for tasks like sentiment analysis \n",
    "    and machine translation\n",
    "    - Vector means array , collection of same kind of data\n",
    "    -Word embeddings not used in ML but is used in DL "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "efab9942",
   "metadata": {},
   "source": [
    "Bag of Words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7875fb30",
   "metadata": {},
   "outputs": [],
   "source": [
    "# BoW Is NLP Technique for Text Modelling specially used to assign Numbers for the text\n",
    "# It a method of feature extraction with text data \n",
    "# Syntactic and Smentic relationship doesn't matter here . Only what matters is the frequency of words \n",
    "# In Programming language we call this as Count Vector\n",
    "# By using BoW we convert Variable-length text into FIxed Length Vector\n",
    "# Why BoW ??\n",
    "# Also because ML works with Numerical Data rather than textual data . Hence BoW\n",
    "# So By using BoW technique we convert a text into its equivalent vector of numbers .\n",
    "\n",
    "# There are 2 Methods : 1-By Scratch      2_ By Algorithm \n",
    "# Bag of Words is the Frequency of each word in a sentence/text \n",
    "# We require Pandas library to save the info in a tabular form"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "eb02d432",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'cat', 'dog', 'eats', 'food', 'red', 'the'}"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sents = \"the red dog cat eats dog dog eats food red cat eats\"\n",
    "set(sents.split())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "3892b297",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import word_tokenize\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "782000cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Aplicable for any langauge\n",
    "sentence_1 = 'it was a good practice for us'\n",
    "sentence_2 = 'it was also good to know about it'\n",
    "sentence_3  = 'for all of us they have done a good practice'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "db096db7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'a',\n",
       " 'about',\n",
       " 'all',\n",
       " 'also',\n",
       " 'done',\n",
       " 'for',\n",
       " 'good',\n",
       " 'have',\n",
       " 'it',\n",
       " 'know',\n",
       " 'of',\n",
       " 'practice',\n",
       " 'they',\n",
       " 'to',\n",
       " 'us',\n",
       " 'was'}"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Find the list of all distinct words present in these 3 sentences. This is my solution . Not very good though \n",
    "\n",
    "set(sentence_1.split() + sentence_2.split() + sentence_3.split())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "03ea9919",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'done', 'it', 'to', 'they', 'practice', 'was', 'us', 'for', 'also', 'about', 'know', 'all', 'of', 'have', 'a', 'good'}\n"
     ]
    }
   ],
   "source": [
    "# Another logic\n",
    "unique_word_list = set(sentence_1.split() + sentence_2.split() + sentence_3.split())\n",
    "print (unique_word_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "57febfd9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'to', 'they', 'it.', 'about', 'know', 'for', 'practice', 'us', 'have', 'also', 'us.', 'done', 'it', 'all', 'of', 'practice.', 'good', 'was', 'a'}\n"
     ]
    }
   ],
   "source": [
    "# If sentences have punctuation marks this will give incorrect output. Split Methos doesnt separate Punctuations !!!\n",
    "sentence_11 = 'it was a good practice for us.'\n",
    "sentence_22 = 'it was also good to know about it.'\n",
    "sentence_33  = 'for all of us they have done a good practice.'\n",
    "unique_word_list = set(sentence_11.split() + sentence_22.split() + sentence_33.split())\n",
    "print (unique_word_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "3c3aee7b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['it', 'was', 'a', 'good', 'practice', 'for', 'us', '.', 'it', 'was', 'also', 'good', 'to', 'know', 'about', 'it', '.', 'for', 'all', 'of', 'us', 'they', 'have', 'done', 'a', 'good', 'practice', '.']\n"
     ]
    }
   ],
   "source": [
    "# so the solution is  WORD TOKENIZATION\n",
    "unique_word_list = word_tokenize(sentence_11) + word_tokenize(sentence_22) + word_tokenize(sentence_33)\n",
    "print (unique_word_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "9378aa7c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['It', 'was', 'a', 'good', 'practice', 'for', 'us', '.', 'It', 'was', 'also', 'good', 'to', 'know', 'about', 'it', '.', 'For', 'all', 'of', 'us', 'they', 'have', 'done', 'a', 'good', 'practice', '.']\n"
     ]
    }
   ],
   "source": [
    "# Now if we have any upper case in the sentence even that is a prob\n",
    "sentence_111 = 'It was a good practice for us.'\n",
    "sentence_222 = 'It was also good to know about it.'\n",
    "sentence_333  = 'For all of us they have done a good practice.'\n",
    "words = word_tokenize(sentence_111) + word_tokenize(sentence_222) + word_tokenize(sentence_333)\n",
    "print (words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "4e341fca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['it', 'was', 'a', 'good', 'practice', 'for', 'us', '.', 'it', 'was', 'also', 'good', 'to', 'know', 'about', 'it', '.', 'for', 'all', 'of', 'us', 'they', 'have', 'done', 'a', 'good', 'practice', '.']\n"
     ]
    }
   ],
   "source": [
    "# Solution\n",
    "# Now if we have any upper case in the sentence even that is a prob\n",
    "sentence_111 = 'It was a good practice for us.'\n",
    "sentence_222 = 'It was also good to know about it.'\n",
    "sentence_333  = 'For all of us they have done a good practice.'\n",
    "wordss = word_tokenize(sentence_111.lower()) + word_tokenize(sentence_222.lower()) + word_tokenize(sentence_333.lower())\n",
    "print (wordss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "8488c23b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'.',\n",
       " 'a',\n",
       " 'about',\n",
       " 'all',\n",
       " 'also',\n",
       " 'done',\n",
       " 'for',\n",
       " 'good',\n",
       " 'have',\n",
       " 'it',\n",
       " 'know',\n",
       " 'of',\n",
       " 'practice',\n",
       " 'they',\n",
       " 'to',\n",
       " 'us',\n",
       " 'was'}"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokens = set(wordss)\n",
    "tokens "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "318328f5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>done</th>\n",
       "      <th>it</th>\n",
       "      <th>to</th>\n",
       "      <th>they</th>\n",
       "      <th>practice</th>\n",
       "      <th>was</th>\n",
       "      <th>us</th>\n",
       "      <th>.</th>\n",
       "      <th>for</th>\n",
       "      <th>also</th>\n",
       "      <th>about</th>\n",
       "      <th>know</th>\n",
       "      <th>all</th>\n",
       "      <th>of</th>\n",
       "      <th>have</th>\n",
       "      <th>a</th>\n",
       "      <th>good</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  done   it   to they practice  was   us    .  for also about know  all   of  \\\n",
       "1  NaN  NaN  NaN  NaN      NaN  NaN  NaN  NaN  NaN  NaN   NaN  NaN  NaN  NaN   \n",
       "2  NaN  NaN  NaN  NaN      NaN  NaN  NaN  NaN  NaN  NaN   NaN  NaN  NaN  NaN   \n",
       "3  NaN  NaN  NaN  NaN      NaN  NaN  NaN  NaN  NaN  NaN   NaN  NaN  NaN  NaN   \n",
       "\n",
       "  have    a good  \n",
       "1  NaN  NaN  NaN  \n",
       "2  NaN  NaN  NaN  \n",
       "3  NaN  NaN  NaN  "
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Now lets create Bag of words with column name as Unique words and the rows will be the sentences \n",
    "df = pd.DataFrame(columns = list(tokens), index = [1,2,3])   # we have changed the SET to LIst here\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "fc447c1b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(['it', 'was', 'a', 'good', 'practice', 'for', 'us', '.'],\n",
       " ['it', 'was', 'also', 'good', 'to', 'know', 'about', 'it', '.'],\n",
       " ['for',\n",
       "  'all',\n",
       "  'of',\n",
       "  'us',\n",
       "  'they',\n",
       "  'have',\n",
       "  'done',\n",
       "  'a',\n",
       "  'good',\n",
       "  'practice',\n",
       "  '.'])"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Now we need the count of each unique words in a sentence\n",
    "tokens1 = word_tokenize(sentence_111.lower())\n",
    "tokens2 = word_tokenize(sentence_222.lower())\n",
    "tokens3 = word_tokenize(sentence_333.lower())\n",
    "# Now using List comprehension lets store the counts \n",
    "tokens1, tokens2, tokens3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5d90e65",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "127c8dc4",
   "metadata": {},
   "outputs": [],
   "source": [
    "counts1 = [tokens1.count(x) for x in df.columns]     # using List comprehension\n",
    "counts2 = [tokens2.count(x) for x in df.columns]\n",
    "counts3 = [tokens3.count(x) for x in df.columns]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "86766720",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "([0, 1, 0, 0, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1],\n",
       " [0, 2, 1, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 1],\n",
       " [1, 0, 0, 1, 1, 0, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1])"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "counts1, counts2, counts3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "b513f99d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>done</th>\n",
       "      <th>it</th>\n",
       "      <th>to</th>\n",
       "      <th>they</th>\n",
       "      <th>practice</th>\n",
       "      <th>was</th>\n",
       "      <th>us</th>\n",
       "      <th>.</th>\n",
       "      <th>for</th>\n",
       "      <th>also</th>\n",
       "      <th>about</th>\n",
       "      <th>know</th>\n",
       "      <th>all</th>\n",
       "      <th>of</th>\n",
       "      <th>have</th>\n",
       "      <th>a</th>\n",
       "      <th>good</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  done it to they practice was us  . for also about know all of have  a good\n",
       "1    0  1  0    0        1   1  1  1   1    0     0    0   0  0    0  1    1\n",
       "2    0  2  1    0        0   1  0  1   0    1     1    1   0  0    0  0    1\n",
       "3    1  0  0    1        1   0  1  1   1    0     0    0   1  1    1  1    1"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Now lets add these values to our DataFrame\n",
    "df.iloc[0,:]=counts1   # df.iloc[0,:]   First row and All the columns\n",
    "df.iloc[1,:]=counts2\n",
    "df.iloc[2,:]=counts3\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a2bd922",
   "metadata": {},
   "source": [
    "If there are hundreds or thousands of rows , then what ?\n",
    "All the above (creating a DataFrame for the Bag of words) can be used Directly using Libraries !!!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "e2bb9d54",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the count vectorizer class\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "# Create the object \n",
    "cv = CountVectorizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "e9fbe1ec",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<3x15 sparse matrix of type '<class 'numpy.int64'>'\n",
       "\twith 22 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Apply the BoW method\n",
    "mydata = cv.fit_transform([sentence_111, sentence_222, sentence_333])\n",
    "mydata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "45290b1b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 1, 0, 0, 1, 1],\n",
       "       [1, 0, 1, 0, 0, 1, 0, 2, 1, 0, 0, 0, 1, 0, 1],\n",
       "       [0, 1, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0]], dtype=int64)"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mydata.toarray()     # This is our required data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "7806c526",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['about', 'all', 'also', 'done', 'for', 'good', 'have', 'it',\n",
       "       'know', 'of', 'practice', 'they', 'to', 'us', 'was'], dtype=object)"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# We got array . now we have to convert it into DataFrame \n",
    "cv.get_feature_names_out()    # This Function Gives the unique words , which will be used for Column Names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "bdcc6d44",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>about</th>\n",
       "      <th>all</th>\n",
       "      <th>also</th>\n",
       "      <th>done</th>\n",
       "      <th>for</th>\n",
       "      <th>good</th>\n",
       "      <th>have</th>\n",
       "      <th>it</th>\n",
       "      <th>know</th>\n",
       "      <th>of</th>\n",
       "      <th>practice</th>\n",
       "      <th>they</th>\n",
       "      <th>to</th>\n",
       "      <th>us</th>\n",
       "      <th>was</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   about  all  also  done  for  good  have  it  know  of  practice  they  to  \\\n",
       "0      0    0     0     0    1     1     0   1     0   0         1     0   0   \n",
       "1      1    0     1     0    0     1     0   2     1   0         0     0   1   \n",
       "2      0    1     0     1    1     1     1   0     0   1         1     1   0   \n",
       "\n",
       "   us  was  \n",
       "0   1    1  \n",
       "1   0    1  \n",
       "2   1    0  "
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Using the above we can create DataFrame \n",
    "df = pd.DataFrame(data=mydata.toarray(), columns = cv.get_feature_names_out()) \n",
    "df   # 0 means the word in not present in that sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9584638b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This ABove os Called Bag of Words Method "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe5943da",
   "metadata": {},
   "source": [
    "# N-grams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9edaa72",
   "metadata": {},
   "outputs": [],
   "source": [
    "# if we have 2 sentences :\n",
    "# This is a good job. I will not miss it for anything\n",
    "# This is not good at all\n",
    "-If we see the BoW for the words 'good,job,miss,not all' both look similar only . \n",
    "-No way to differentiate whihc is +ve and which -ve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a59a7c41",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This means Good +Not must come together and we should find the Frequency of \"not good\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42459db2",
   "metadata": {},
   "source": [
    "An N-Gram is an N-token sequence of words: a 2-gram(commpnly called a Bigram) is a 2-word sequence of words like\n",
    "'really good', 'not good', 'your homework'... And a 3-gram (commonly called a Tri-gram) is a 3 word sequence like \"not at all\",\n",
    "'turn off light'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f88e6e2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# So we use Bi-Grams(Bag of Bigrams). The model can differentiate between a negative and a postive sentence "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df8f4de9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the object \n",
    "cv = CountVectorizer(ngram_range=(1,2))    # Minimum 1 word and Maximum 2 words\n",
    "\n",
    "# Apply the BoW N-Gram method \n",
    "mydata = cv.fit_transform([sentence_111, sentence_222, sentence_333])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92e1b66b",
   "metadata": {},
   "outputs": [],
   "source": [
    "cv.get_feature_names_out()     # cv is CountVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48315be6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now Print the final DataFrame\n",
    "df = pd.DataFrame(data=mydata.toarray(), columns = cv.get_feature_names_out())\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3b6aeae",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mostly we ll use Bigram or Trigram ."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1701fd76",
   "metadata": {},
   "source": [
    "# TF-IDF Vectorizer    Term frequency "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d9834b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TF is concern with a single line/ sentence \n",
    "# IDF is about the whole document "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d26bfd6d",
   "metadata": {},
   "source": [
    "TF(t) = (Number of times Term t appears in a document)/ Total number of items in the document"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec9db40c",
   "metadata": {},
   "source": [
    "IDF(t) = log_e(Toal NUmber of Documents/Number of Documents with Term t in it) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0c24af9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TF-IDF = term x within Document Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "32cde8af",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\User\\anaconda3\\lib\\site-packages\\scipy\\__init__.py:155: UserWarning: A NumPy version >=1.18.5 and <1.25.0 is required for this version of SciPy (detected version 1.26.4\n",
      "  warnings.warn(f\"A NumPy version >={np_minversion} and <{np_maxversion}\"\n"
     ]
    }
   ],
   "source": [
    "# import the class \n",
    "from sklearn.feature_extraction.text import TfidfVectorizer "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "6b533fca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the object \n",
    "tfidf = TfidfVectorizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "60308b96",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply the algoritm \n",
    "new_data = tfidf.fit_transform([sentence_111, sentence_222, sentence_333])  # all sentences in 1 list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "e7390253",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.        , 0.        , 0.        , 0.        , 0.42246056,\n",
       "        0.32807831, 0.        , 0.42246056, 0.        , 0.        ,\n",
       "        0.42246056, 0.        , 0.        , 0.42246056, 0.42246056],\n",
       "       [0.37162591, 0.        , 0.37162591, 0.        , 0.        ,\n",
       "        0.21948825, 0.        , 0.56526217, 0.37162591, 0.        ,\n",
       "        0.        , 0.        , 0.37162591, 0.        , 0.28263108],\n",
       "       [0.        , 0.37571621, 0.        , 0.37571621, 0.28574186,\n",
       "        0.22190405, 0.37571621, 0.        , 0.        , 0.37571621,\n",
       "        0.28574186, 0.37571621, 0.        , 0.28574186, 0.        ]])"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_data.toarray()  # Give the array of all the TF-IDF for the whiole doc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "06e6a350",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['about', 'all', 'also', 'done', 'for', 'good', 'have', 'it',\n",
       "       'know', 'of', 'practice', 'they', 'to', 'us', 'was'], dtype=object)"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# To get the names of the columns \n",
    "tfidf.get_feature_names_out()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "6104e8e6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>about</th>\n",
       "      <th>all</th>\n",
       "      <th>also</th>\n",
       "      <th>done</th>\n",
       "      <th>for</th>\n",
       "      <th>good</th>\n",
       "      <th>have</th>\n",
       "      <th>it</th>\n",
       "      <th>know</th>\n",
       "      <th>of</th>\n",
       "      <th>practice</th>\n",
       "      <th>they</th>\n",
       "      <th>to</th>\n",
       "      <th>us</th>\n",
       "      <th>was</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.422461</td>\n",
       "      <td>0.328078</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.422461</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.422461</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.422461</td>\n",
       "      <td>0.422461</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.371626</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.371626</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.219488</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.565262</td>\n",
       "      <td>0.371626</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.371626</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.282631</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.375716</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.375716</td>\n",
       "      <td>0.285742</td>\n",
       "      <td>0.221904</td>\n",
       "      <td>0.375716</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.375716</td>\n",
       "      <td>0.285742</td>\n",
       "      <td>0.375716</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.285742</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      about       all      also      done       for      good      have  \\\n",
       "0  0.000000  0.000000  0.000000  0.000000  0.422461  0.328078  0.000000   \n",
       "1  0.371626  0.000000  0.371626  0.000000  0.000000  0.219488  0.000000   \n",
       "2  0.000000  0.375716  0.000000  0.375716  0.285742  0.221904  0.375716   \n",
       "\n",
       "         it      know        of  practice      they        to        us  \\\n",
       "0  0.422461  0.000000  0.000000  0.422461  0.000000  0.000000  0.422461   \n",
       "1  0.565262  0.371626  0.000000  0.000000  0.000000  0.371626  0.000000   \n",
       "2  0.000000  0.000000  0.375716  0.285742  0.375716  0.000000  0.285742   \n",
       "\n",
       "        was  \n",
       "0  0.422461  \n",
       "1  0.282631  \n",
       "2  0.000000  "
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Now Lets craete a New dataFrame to get our desired data with correct column names and the data \n",
    "df = pd.DataFrame(data=new_data.toarray(), \n",
    "                  columns = tfidf.get_feature_names_out())\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "241b1d12",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We saw Tf-Idf to convert text data into numerical format"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1320ecbc",
   "metadata": {},
   "source": [
    "# Machine Learning "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f163b2c7",
   "metadata": {},
   "source": [
    " Decision Tree (we ll see classification here)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "c2ead625",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Binary Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "946a1210",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dataset : "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
